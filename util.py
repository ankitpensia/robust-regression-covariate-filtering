# This file contains helper functions pertaining to linear algebra, random sampling, and plotting.

import numpy as np
import matplotlib.pyplot as plt
from scipy.linalg import eigh
from scipy.sparse.linalg import eigsh
import random
import string
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import cm
from matplotlib.ticker import LinearLocator, FormatStrFormatter
from abc import ABC, abstractmethod
from tqdm import tqdm
from tqdm.contrib.concurrent import process_map  # or thread_map
import pandas as pd
from cycler import cycler

markers = [".",",","o","v","^","<",">","1","2","3","4","8","s","p","P","*","h","H","+","x","X","D","d","|","_"]


_DEFAULT_CORES = 8

def sample_heavy_covariates(n, d, pareto_param=4):
    """Generate multiple samples from a centered isotropic heavy-tailed distribution (4-Pareto)

    First, a matrix X' \in R^{n x d} is generated whose each coordinate is iid 
    with pdf proportional to $1/(1+x)^{a+1}$ for $x\geq 0$, where a is the pareto_param.

    The final output is generated by symmetrizing each entry of this matrix by random signs. 

    Args:
        n (int): number of samples
        d (int): number of features
        pareto_param: The parameter for the pareto distribution

    Returns:
        numpy.ndarray: A [n,d] dimensional matrix X, 
    """
    X = np.random.pareto(pareto_param, [n, d])
    X = X * (2 * np.random.binomial(1, 0.5, [n, d]) - 1)
    return X


def sample_heavy_response(n,pareto_param=2):
    """Generate heavy-tailed independent noises for the linear regression (Pareto family).

    Each entry of the output is sampled with pdf proportional to 1/(1+|x|)^a, where a is the pareto_param.

    Args:
        n (int): number of samples
    Returns:
        numpy.ndarray: A [n] dimensional array
    """
    eps = np.random.pareto(pareto_param, [n])
    eps = eps * (2 * np.random.binomial(1, 0.5, [n]) - 1)
    return eps


def leading_eigenvector(X, w, method=None):
    """Computes the leading eigenvector of the covariance matrix of data X.

    @TODO:
        1. Improve the runtime with power iteration that doesn't require
            calculating the covariance matrix explicitly.

    Args:
        X (np.ndarray): (n,d) dimensional data
        w ([np.ndarray]): Weight of each data -
                        can be both discrete or continuous
        method ([string], optional): The method used to compute the eigenvector
                            . Defaults to None.

    Raises:
        Exception: If the method is not implemented

    Returns:
        [tuple]: A pair of (eigenval,eigenvector)
    """
    n, d = X.shape
    if method == "power_iter":
        raise Exception("Need to implement power iteration")
    if not np.all(w >= 0):
        raise Exception("Weights need to be positive")
    if method == "scipy_eigh_brute":
        w_cov, w_mean = weighted_covariance(X, w)
        eigval, eigvec = eigh(w_cov, eigvals=(d - 1, d - 1))
        return eigval, eigvec, w_mean
    if method == "scipy_eigsh":
        # This is analogous to power iteration (but after constructing the covariance matrix explicitly)
        w_cov, w_mean = weighted_covariance(X, w)
        eigval, eigvec = eigsh(w_cov, k=1)
        return eigval, eigvec, w_mean
    raise Exception("Not implemented")


def weighted_mean(X, w):
    """Returns the weighted mean of the data X w.r.t. weights w
    Args:
        X ([ndarray]): A matrix of size (n,d)
        w ([ndarray]): A non-negative vector of size n

    Returns:
        [ndarray]: A ndarray of size (d) defiend as (\sum_{i=1}^n w_i X_i)/ \|w\|_1
    """
    return X.T.dot(w) / w.sum()


def weighted_covariance(X, w):
    """Returns the weighted mean of the data X w.r.t. weights w
    @TODO:
        1. Resolve the conflict between np.cov(X.T) and this function with uniform weights

    Args:
        X ([ndarray]): A matrix of size (n,d)
        w ([ndarray]): A non-negative vector of size n

    Returns:
        ([ndarray],[ndarray]): A tuple of weighted_covariance (of d x d) and weighted_mean (of d)

         w_cov = (\sum_{i=1}^n w_i X_iX_i)^T/ \|w\|_1 - mu_w mu_w^T, where
         mu_w =(\sum_{i=1}^n w_i X_i)^T/ \|w\|_1/.
    """
    w_mean = weighted_mean(X, w)
    w_second_moment = (X * w[:, None]).T.dot(X) / w.sum()
    w_cov = w_second_moment - w_mean[:, None].dot(w_mean[None, :])
    return w_cov, w_mean


def rand_string(length):
    return "".join(random.choices(string.ascii_uppercase + string.digits, k=length))


def plot_error_cdf(results_pd):
    res_sort = results_pd.transform(np.sort)
    fig = plt.figure()
    num_exp = res_sort.shape[0]
    prob = np.arange(num_exp) / num_exp
    for column in res_sort.columns:
        if column == "n" or column == "d":
            continue
        plt.plot(res_sort[column], prob, label=column)
    plt.legend()
    plt.xlabel("Error")
    plt.ylabel("CDF")
    plt.title("CDF of error quantile")


def plot_quantiles(
    results_pd, show_max=False, annotate=False, linewidth=3, y_lim=None, y_min=0, title=""
):
    res_sort = results_pd.transform(np.sort)
    fig = plt.figure(figsize=(10, 7))
    # ax.set_prop_cycle(default_cycler)
    num_exp = res_sort.shape[0]
    delta = 1 - np.arange(num_exp) / num_exp
    labels = []
    if annotate:
        axis = plt.gca()
    for i, column in enumerate(res_sort.columns):
        if column == "n" or column == "d":
            continue
        if show_max is False:
            label = column
        else:
            label = f"{column}. MaxErr: {res_sort[column].max():.3f}"
        labels.append(label)
        plt.plot(
            np.log2(1 / delta),
            res_sort[column],
            label=label,
            alpha=0.8,
            linewidth=linewidth,
        )
    plt.xlabel("log2(1/delta):")
    plt.ylabel("Error at confidence 1 - \delta")
    plt.title(f"Error as function of \log(1/\delta) {title}")
    if annotate:
        ax = plt.gca()
        for line, name in zip(ax.lines, labels):
            y = line.get_ydata()[-1]
            ax.annotate(
                name,
                xy=(1, y),
                xytext=(6, 0),
                color=line.get_color(),
                xycoords=ax.get_yaxis_transform(),
                textcoords="offset points",
                size=14,
                va="center",
            )
    if y_lim:
        plt.ylim(top=y_lim, bottom=y_min)
    plt.legend(bbox_to_anchor=(1.05, 1.0), loc="upper left")
    plt.tight_layout()
    plt.show()


def compare_linear_regressor(X, y, estimators, true_beta):
    num_of_methods = len(estimators)
    est_names = [est.name for est in estimators]
    error_dict = dict.fromkeys(est_names, 0.0)
    for est in estimators:
        err = est.estimateError(X, y, true_beta)
        error_dict[est.name] = err
    return error_dict


def sample_n_compare_regression_single(
    estimators,
    n,
    d, 
    data_dist_dict=None
):
    np.random.seed()
    if data_dist_dict.get('name') is None or data_dist_dict.get('name') == "heavy" or data_dist_dict.get('name') == "heavy+heavy":
        X = sample_heavy_covariates(n, d, pareto_param= data_dist_dict.get('pareto_param_x' ))
        eps = sample_heavy_response(n,pareto_param= data_dist_dict.get('pareto_param_y' ))
        beta_star = np.random.randn(d)
        beta_star = beta_star / np.linalg.norm(beta_star)
        y = X.dot(beta_star) + eps
    elif data_dist_dict.get('name') == "normal+normal":
        X = np.random.randn(n, d)
        eps = np.random.randn(n)
        beta_star = np.random.randn(d)
        beta_star = beta_star / np.linalg.norm(beta_star)
        y = X.dot(beta_star) + eps
    elif data_dist_dict.get('name') == "normal+heavy":
        X = np.random.randn(n, d)
        eps = sample_heavy_response(n)
        beta_star = np.random.randn(d)
        beta_star = beta_star / np.linalg.norm(beta_star)
        y = X.dot(beta_star) + eps
    elif data_dist_dict.get('name') == "outlliers_reg":
        X = sample_heavy_covariates(n, d, pareto_param= data_dist_dict.get('pareto_param_x' ))
        eps = sample_heavy_response(n, pareto_param= data_dist_dict.get('pareto_param_y' ))
        x_scale = data_dist_dict.get('x_scale')
        y_x_ratio = data_dist_dict.get('y_x_ratio')
        outliers_frac = data_dist_dict.get('outliers_frac')


        outliers_1 = int(n * outliers_frac * 0.5)
        outliers_2 = int(n * outliers_frac * 0.5)

        X[n - outliers_1 - outliers_2 : n - n - outliers_1, :] = np.ones((outliers_1, d)) * (x_scale)
        beta_star = np.random.randn(d)
        beta_star = beta_star / np.linalg.norm(beta_star)
        y = X.dot(beta_star) + eps
        y[n - outliers_1 - outliers_2 :] = y_x_ratio * x_scale
    else:
        print(data_dist_dict)
        raise NotImplementedError
    error_dict = compare_linear_regressor(X, y, estimators, beta_star)
    error_dict["n"] = n
    error_dict["d"] = d
    return error_dict


def sample_n_compare_regression_single_unpack(args):
    return sample_n_compare_regression_single(*args)


def run_experiment_parallel(num_exp, estimators, n, d, data_dist_dict, cores=8):
    if cores is None:
        cores = mp.cpu_count() - 1
    jobs = [(estimators, n, d, data_dist_dict)] * num_exp
    errors_list = process_map(
        sample_n_compare_regression_single_unpack, jobs, max_workers=cores, chunksize=1)
    # with mp.Pool(processes = cores) as pool:
    #     errors_list = pool.starmap(sample_n_compare_mean_single_unpack, jobs)
    error_pd = pd.DataFrame(errors_list)
    return error_pd


def run_experiment_serial(num_exp, estimators, n, d, data_dist, pareto_param=4):
    errors_list = []
    for i in tqdm(range(num_exp)):
        err = sample_n_compare_regression_single(
            estimators, n, d, data_dist, pareto_param
        )
        errors_list.append(err)
    error_pd = pd.DataFrame(errors_list)
    return error_pd


class EstimatorPlot:
    def __init__(self, name):
        self.name = name
        self.process_pd(name)

    def process_pd(self, name):
        if name == "n" or name == "d":
            self.type = None

        if "TheilSen" in name:
            self.type = "TS"
        if "Ransac" in name:
            self.type = "RS"

        if "LeastSquares" in name:
            self.type = "OLS"

        if "F_steps" in name:
            self.filter = True
            self.filter_steps = int(name[name.find("F_steps") :].split("_")[2])
        else:
            self.filter = False
            self.filter_steps = 0

        if "LTS" in name:
            self.type = "LTS"
            self.hard_threshold = int(name[name.find("HT") :].split("_")[1])
            # self.stop_thres = float(name[name.find("stop_thres"):].split("_")[2])
        else:
            self.hard_threshold = 0
            self.stop_thres = 0

        if "Hub" in name:
            self.type = "Huber"
            self.huber_thres = float(name[name.find("thres") :].split("_")[1])
        else:
            self.huber_thres = 0

def plot_error_two(logs, title=""):
    fig, (ax2, ax1) = plt.subplots(1, 2, figsize=(15, 8))
    for est, logs_est in logs.items():
        steps = logs_est.shape[0]
        arr = np.arange(steps)

        ax1.plot(arr, logs_est[:, 0], label=f"Min Fx: {logs_est[-1,0]:0.3f}--- {est}")
        ax1.set_title(f"{title}. Function value")
        ax1.set_xlabel("Iters")
        ax1.set_yscale("log")
        ax1.legend(bbox_to_anchor=(0.5, 1.0), loc="upper center")

        ax2.plot(
            arr, logs_est[:, 1], label=f"Min L2 Error: {logs_est[-1,1]:0.3f}---{est}"
        )
        ax2.set_title(f"{title}. L2 Estimation error")
        ax2.set_xlabel("Iters")
        ax2.legend(bbox_to_anchor=(0.55, 1.0), loc="upper center")
        ax2.legend()
        plt.tight_layout()
